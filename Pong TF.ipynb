{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "D = 80 * 80 # Dimision of input image\n",
    "H = 200 # Number of hidden layer neurons\n",
    "batch_size = 10 # Every how many episodes to do a param update\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # Discount factor for reward\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad^2\n",
    "render = True\n",
    "save_path = 'models/pong.ckpt'\n",
    "MAX_EPISODE_NUMBER = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward\"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: \n",
    "            running_add = 0 # reset the sum, since this was a game boundary\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\"Prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector\"\"\"\n",
    "    I = I[35:195] # Crop\n",
    "    I = I[::2, ::2, 0] # Downsample by factor of 2\n",
    "    I[I == 144] = 0 # Erase background\n",
    "    I[I == 109] = 0 # Erase background\n",
    "    I[I != 0] = 1 # everything else just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "W1 = tf.Variable(tf.truncated_normal([D, H], mean=0, stddev=1./np.sqrt(D), dtype=tf.float32))\n",
    "W2 = tf.Variable(tf.truncated_normal([H, 1], mean=0, stddev=1./np.sqrt(H), dtype=tf.float32))\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, D])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "discounted_rewards = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "fc1 = tf.matmul(x, W1)\n",
    "relu = tf.nn.relu(fc1)\n",
    "fc2 = tf.matmul(relu, W2)\n",
    "# Calculate probability which is used for sample action\n",
    "sig = tf.nn.sigmoid(fc2)\n",
    "# Train the policy network according the reward we get in final\n",
    "loss = tf.nn.l2_loss(y - sig)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay_rate)\n",
    "grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=discounted_rewards)\n",
    "train = optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 18:01:17,872] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-57-79a763210b8f>:13: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 18:01:19,015] From <ipython-input-57-79a763210b8f>:13: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pong.ckpt-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 18:01:19,200] Restoring parameters from models/pong.ckpt-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model: models/pong.ckpt-100\n",
      "WARNING:tensorflow:From <ipython-input-57-79a763210b8f>:25: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 18:01:19,666] From <ipython-input-57-79a763210b8f>:25: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 101: reward: -21.0, mean reward: -0.210000\n",
      "ep 102: reward: -42.0, mean reward: -0.627900\n",
      "ep 103: reward: -63.0, mean reward: -1.251621\n",
      "ep 104: reward: -84.0, mean reward: -2.079105\n",
      "ep 105: reward: -104.0, mean reward: -3.098314\n",
      "ep 106: reward: -125.0, mean reward: -4.317331\n",
      "ep 107: reward: -145.0, mean reward: -5.724157\n",
      "ep 108: reward: -166.0, mean reward: -7.326916\n",
      "ep 109: reward: -187.0, mean reward: -9.123647\n",
      "ep 110: reward: -207.0, mean reward: -11.102410\n",
      "ep 111: reward: -228.0, mean reward: -13.271386\n",
      "ep 112: reward: -249.0, mean reward: -15.628672\n",
      "ep 113: reward: -270.0, mean reward: -18.172385\n",
      "ep 114: reward: -291.0, mean reward: -20.900662\n",
      "ep 115: reward: -312.0, mean reward: -23.811655\n",
      "ep 116: reward: -333.0, mean reward: -26.903538\n",
      "ep 117: reward: -353.0, mean reward: -30.164503\n",
      "ep 118: reward: -374.0, mean reward: -33.602858\n",
      "ep 119: reward: -395.0, mean reward: -37.216829\n",
      "ep 120: reward: -416.0, mean reward: -41.004661\n",
      "ep 121: reward: -436.0, mean reward: -44.954615\n",
      "ep 122: reward: -457.0, mean reward: -49.075068\n",
      "ep 123: reward: -476.0, mean reward: -53.344318\n",
      "ep 124: reward: -497.0, mean reward: -57.780874\n",
      "ep 125: reward: -517.0, mean reward: -62.373066\n",
      "ep 126: reward: -538.0, mean reward: -67.129335\n",
      "ep 127: reward: -558.0, mean reward: -72.038042\n",
      "ep 128: reward: -578.0, mean reward: -77.097661\n",
      "ep 129: reward: -599.0, mean reward: -82.316685\n",
      "ep 130: reward: -619.0, mean reward: -87.683518\n",
      "ep 131: reward: -640.0, mean reward: -93.206683\n",
      "ep 132: reward: -661.0, mean reward: -98.884616\n",
      "ep 133: reward: -681.0, mean reward: -104.705770\n",
      "ep 134: reward: -702.0, mean reward: -110.678712\n",
      "ep 135: reward: -723.0, mean reward: -116.801925\n",
      "ep 136: reward: -744.0, mean reward: -123.073906\n",
      "ep 137: reward: -765.0, mean reward: -129.493167\n",
      "ep 138: reward: -786.0, mean reward: -136.058235\n",
      "ep 139: reward: -806.0, mean reward: -142.757653\n",
      "ep 140: reward: -827.0, mean reward: -149.600076\n",
      "ep 141: reward: -848.0, mean reward: -156.584075\n",
      "ep 142: reward: -869.0, mean reward: -163.708235\n",
      "ep 143: reward: -889.0, mean reward: -170.961152\n",
      "ep 144: reward: -909.0, mean reward: -178.341541\n",
      "ep 145: reward: -930.0, mean reward: -185.858125\n",
      "ep 146: reward: -951.0, mean reward: -193.509544\n",
      "ep 147: reward: -972.0, mean reward: -201.294449\n",
      "ep 148: reward: -993.0, mean reward: -209.211504\n",
      "ep 149: reward: -1014.0, mean reward: -217.259389\n",
      "ep 150: reward: -1034.0, mean reward: -225.426795\n",
      "SAVE MODEL #150\n",
      "ep 151: reward: -1055.0, mean reward: -233.722527\n",
      "ep 152: reward: -1076.0, mean reward: -242.145302\n",
      "ep 153: reward: -1097.0, mean reward: -250.693849\n",
      "ep 154: reward: -1118.0, mean reward: -259.366910\n",
      "ep 155: reward: -1137.0, mean reward: -268.143241\n",
      "ep 156: reward: -1158.0, mean reward: -277.041809\n",
      "ep 157: reward: -1179.0, mean reward: -286.061391\n",
      "ep 158: reward: -1200.0, mean reward: -295.200777\n",
      "ep 159: reward: -1221.0, mean reward: -304.458769\n",
      "ep 160: reward: -1241.0, mean reward: -313.824181\n",
      "ep 161: reward: -1261.0, mean reward: -323.295940\n",
      "ep 162: reward: -1282.0, mean reward: -332.882980\n",
      "ep 163: reward: -1303.0, mean reward: -342.584150\n",
      "ep 164: reward: -1324.0, mean reward: -352.398309\n",
      "ep 165: reward: -1344.0, mean reward: -362.314326\n",
      "ep 166: reward: -1364.0, mean reward: -372.331183\n",
      "ep 167: reward: -1385.0, mean reward: -382.457871\n",
      "ep 168: reward: -1406.0, mean reward: -392.693292\n",
      "ep 169: reward: -1427.0, mean reward: -403.036359\n",
      "ep 170: reward: -1447.0, mean reward: -413.475996\n",
      "ep 171: reward: -1468.0, mean reward: -424.021236\n",
      "ep 172: reward: -1489.0, mean reward: -434.671023\n",
      "ep 173: reward: -1510.0, mean reward: -445.424313\n",
      "ep 174: reward: -1530.0, mean reward: -456.270070\n",
      "ep 175: reward: -1551.0, mean reward: -467.217369\n",
      "ep 176: reward: -1572.0, mean reward: -478.265195\n",
      "ep 177: reward: -1592.0, mean reward: -489.402544\n",
      "ep 178: reward: -1613.0, mean reward: -500.638518\n",
      "ep 179: reward: -1634.0, mean reward: -511.972133\n",
      "ep 180: reward: -1654.0, mean reward: -523.392412\n",
      "ep 181: reward: -1675.0, mean reward: -534.908487\n",
      "ep 182: reward: -1696.0, mean reward: -546.519403\n",
      "ep 183: reward: -1717.0, mean reward: -558.224209\n",
      "ep 184: reward: -1737.0, mean reward: -570.011966\n",
      "ep 185: reward: -1758.0, mean reward: -581.891847\n",
      "ep 186: reward: -1779.0, mean reward: -593.862928\n",
      "ep 187: reward: -1799.0, mean reward: -605.914299\n",
      "ep 188: reward: -1820.0, mean reward: -618.055156\n",
      "ep 189: reward: -1841.0, mean reward: -630.284604\n",
      "ep 190: reward: -1862.0, mean reward: -642.601758\n",
      "ep 191: reward: -1882.0, mean reward: -654.995741\n",
      "ep 192: reward: -1902.0, mean reward: -667.465783\n",
      "ep 193: reward: -1923.0, mean reward: -680.021126\n",
      "ep 194: reward: -1941.0, mean reward: -692.630914\n",
      "ep 195: reward: -1962.0, mean reward: -705.324605\n",
      "ep 196: reward: -1983.0, mean reward: -718.101359\n",
      "ep 197: reward: -2004.0, mean reward: -730.960346\n",
      "ep 198: reward: -2024.0, mean reward: -743.890742\n",
      "ep 199: reward: -2043.0, mean reward: -756.881835\n",
      "ep 200: reward: -2064.0, mean reward: -769.953016\n",
      "SAVE MODEL #200\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None\n",
    "xs, ys, drs = [], [], []\n",
    "reward_sum = 0\n",
    "running_reward = 0\n",
    "episode_number = 0\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "load_was_success = True\n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except Exception:\n",
    "    print(\"No saved model to load, starting new session\")\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print(\"Load model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    episode_number = int(load_path.split('-')[-1])\n",
    "\n",
    "\n",
    "while True:\n",
    "    if episode_number >= MAX_EPISODE_NUMBER:\n",
    "        break\n",
    "\n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    # process the observation, set input to network to be difference image \n",
    "    cur_x = prepro(observation)\n",
    "    tf_x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    tf_x = np.reshape(tf_x, (1, -1))\n",
    "    prev_x = cur_x\n",
    "    \n",
    "    feed_dict={x: tf_x}\n",
    "    aprob = sess.run(sig, feed_dict=feed_dict)\n",
    "    action = 2 if np.random.uniform() < aprob else 3\n",
    "    \n",
    "    # record various intermediates\n",
    "    xs.append(tf_x)\n",
    "    tf_y = 1 if action == 2 else 0\n",
    "    ys.append(tf_y)\n",
    "    \n",
    "    # step the enviornment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    drs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        episode_number += 1\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        \n",
    "        # stack everything together\n",
    "        epx = np.vstack(xs)\n",
    "        epy = np.vstack(ys)\n",
    "        epr = np.vstack(drs)\n",
    "        xs, ys, drs = [], [], []\n",
    "        \n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "        \n",
    "        feed_dict = {x: epx, y: epy, discounted_rewards: discounted_epr}\n",
    "        sess.run(train, feed_dict=feed_dict)\n",
    "        \n",
    "        observation  = env.reset()\n",
    "        prev_x = None\n",
    "        \n",
    "        if episode_number % 10 == 0:\n",
    "            print('ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward)) \n",
    "        \n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess, save_path, global_step=episode_number)\n",
    "            print(\"SAVE MODEL #{}\".format(episode_number))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
