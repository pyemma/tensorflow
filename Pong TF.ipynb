{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "D = 80 * 80 # Dimision of input image\n",
    "H = 200 # Number of hidden layer neurons\n",
    "batch_size = 10 # Every how many episodes to do a param update\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # Discount factor for reward\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad^2\n",
    "render = True\n",
    "save_path = 'models/pong.ckpt'\n",
    "MAX_EPISODE_NUMBER = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward\"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: \n",
    "            running_add = 0 # reset the sum, since this was a game boundary\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\"Prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector\"\"\"\n",
    "    I = I[35:195] # Crop\n",
    "    I = I[::2, ::2, 0] # Downsample by factor of 2\n",
    "    I[I == 144] = 0 # Erase background\n",
    "    I[I == 109] = 0 # Erase background\n",
    "    I[I != 0] = 1 # everything else just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "W1 = tf.Variable(tf.truncated_normal([D, H], mean=0, stddev=1./np.sqrt(D), dtype=tf.float32))\n",
    "W2 = tf.Variable(tf.truncated_normal([H, 1], mean=0, stddev=1./np.sqrt(H), dtype=tf.float32))\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, D])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "discounted_rewards = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "fc1 = tf.matmul(x, W1)\n",
    "relu = tf.nn.relu(fc1)\n",
    "fc2 = tf.matmul(relu, W2)\n",
    "# Calculate probability which is used for sample action\n",
    "sig = tf.nn.sigmoid(fc2)\n",
    "# Train the policy network according the reward we get in final\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=fc2)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay_rate)\n",
    "grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=discounted_rewards)\n",
    "train = optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 18:23:24,477] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-59-46bb681f6b45>:13: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 18:23:26,118] From <ipython-input-59-46bb681f6b45>:13: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model to load, starting new session\n",
      "ep 10: reward: -206.0, mean reward: -11.072610\n",
      "ep 20: reward: -412.0, mean reward: -40.773584\n",
      "ep 30: reward: -619.0, mean reward: -87.352579\n",
      "ep 40: reward: -816.0, mean reward: -148.556337\n",
      "ep 50: reward: -1016.0, mean reward: -222.937719\n",
      "SAVE MODEL #50\n",
      "ep 60: reward: -1221.0, mean reward: -309.611854\n",
      "ep 70: reward: -1429.0, mean reward: -407.869212\n",
      "ep 80: reward: -1632.0, mean reward: -516.310582\n",
      "ep 90: reward: -1837.0, mean reward: -634.072803\n",
      "ep 100: reward: -2040.0, mean reward: -760.019591\n",
      "SAVE MODEL #100\n",
      "ep 110: reward: -2239.0, mean reward: -892.960051\n",
      "ep 120: reward: -2445.0, mean reward: -1032.767027\n",
      "ep 130: reward: -2652.0, mean reward: -1178.866655\n",
      "ep 140: reward: -2851.0, mean reward: -1330.297020\n",
      "ep 150: reward: -3055.0, mean reward: -1486.670764\n",
      "SAVE MODEL #150\n",
      "ep 160: reward: -3262.0, mean reward: -1647.772083\n",
      "ep 170: reward: -3465.0, mean reward: -1812.932743\n",
      "ep 180: reward: -3671.0, mean reward: -1982.021018\n",
      "ep 190: reward: -3875.0, mean reward: -2154.550499\n",
      "ep 200: reward: -4080.0, mean reward: -2330.100857\n",
      "SAVE MODEL #200\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None\n",
    "xs, ys, drs = [], [], []\n",
    "reward_sum = 0\n",
    "running_reward = 0\n",
    "episode_number = 0\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "load_was_success = True\n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except Exception:\n",
    "    print(\"No saved model to load, starting new session\")\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print(\"Load model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    episode_number = int(load_path.split('-')[-1])\n",
    "\n",
    "\n",
    "while True:\n",
    "    if episode_number >= MAX_EPISODE_NUMBER:\n",
    "        break\n",
    "\n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    # process the observation, set input to network to be difference image \n",
    "    cur_x = prepro(observation)\n",
    "    tf_x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    tf_x = np.reshape(tf_x, (1, -1))\n",
    "    prev_x = cur_x\n",
    "    \n",
    "    feed_dict={x: tf_x}\n",
    "    aprob = sess.run(sig, feed_dict=feed_dict)\n",
    "    action = 2 if np.random.uniform() < aprob else 3\n",
    "    \n",
    "    # record various intermediates\n",
    "    xs.append(tf_x)\n",
    "    tf_y = 1 if action == 2 else 0\n",
    "    ys.append(tf_y)\n",
    "    \n",
    "    # step the enviornment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    drs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        episode_number += 1\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        \n",
    "        # stack everything together\n",
    "        epx = np.vstack(xs)\n",
    "        epy = np.vstack(ys)\n",
    "        epr = np.vstack(drs)\n",
    "        xs, ys, drs = [], [], []\n",
    "        \n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "        \n",
    "        feed_dict = {x: epx, y: epy, discounted_rewards: discounted_epr}\n",
    "        sess.run(train, feed_dict=feed_dict)\n",
    "        \n",
    "        observation  = env.reset()\n",
    "        prev_x = None\n",
    "        \n",
    "        if episode_number % 10 == 0:\n",
    "            print('ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward)) \n",
    "        \n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess, save_path, global_step=episode_number)\n",
    "            print(\"SAVE MODEL #{}\".format(episode_number))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
