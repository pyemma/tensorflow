{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_lambda(scores, labels):\n",
    "    assert(scores.shape[0] == labels.shape[0])\n",
    "    \n",
    "    N = scores.shape[0]\n",
    "    \n",
    "    A = np.tile(scores, N)\n",
    "    scores_diff = A - A.T\n",
    "\n",
    "    B = np.tile(labels, N)\n",
    "    labels_diff = B - B.T\n",
    "    labels_diff = (labels_diff > 0).astype(np.float)\n",
    "    np.fill_diagonal(labels_diff, 0.0)\n",
    "\n",
    "    sigmoid_like = -1.0 / (1.0 + np.exp(scores_diff))\n",
    "    \n",
    "    sorted_labels = labels.copy()\n",
    "    sorted_labels[::-1].sort(axis=0)\n",
    "    max_dcg = np.sum((np.power(2, sorted_labels) - 1) / np.log(np.arange(N) + 2).reshape((N, 1)))\n",
    "    if max_dcg == 0:\n",
    "        return torch.tensor((N, 1), dtype=torch.float)\n",
    "\n",
    "    DCG = np.tile(np.power(2, labels) - 1, N) / np.tile(np.log(np.arange(N) + 2), (N, 1))\n",
    "\n",
    "    delta_dcg = DCG + DCG.T - np.tile(DCG.diagonal().reshape((N, 1)), N) - np.tile(DCG.diagonal(), (N, 1))\n",
    "    delta_dcg = np.abs(delta_dcg)\n",
    "    \n",
    "    G = sigmoid_like * labels_diff * delta_dcg / max_dcg\n",
    "    gradients = np.sum(G - G.T, axis=1).reshape((N, 1))\n",
    "    \n",
    "    return torch.tensor(gradients, dtype=torch.float)\n",
    "\n",
    "def compute_ndcg(scores, labels):\n",
    "    assert(scores.shape[0] == labels.shape[0])\n",
    "    \n",
    "    N = scores.shape[0]\n",
    "    \n",
    "    sorted_labels = labels.copy()\n",
    "    sorted_labels[::-1].sort(axis=0)\n",
    "    max_dcg = np.sum((np.power(2, sorted_labels) - 1) / np.log(np.arange(N) + 2).reshape((N, 1)))\n",
    "    \n",
    "    idx = np.flip(np.argsort(scores, axis=0).reshape((N,)), axis=0)\n",
    "    true_labels = labels[idx]\n",
    "    true_dcg = np.sum((np.power(2, true_labels) - 1) / np.log(np.arange(N) + 2).reshape((N, 1)))\n",
    "    \n",
    "    return torch.tensor(true_dcg / max_dcg, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LambdaLoss(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target):\n",
    "        # input is scores, target is labels\n",
    "        input, target = input.detach(), target.detach()\n",
    "        ctx.save_for_backward(input, target)\n",
    "        return compute_ndcg(input.numpy(), target.numpy())\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input, target = ctx.saved_tensors\n",
    "        grad = compute_lambda(input.numpy(), target.numpy())\n",
    "        \n",
    "        return grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Sample = namedtuple('Sample', ('label', 'feature'))\n",
    "\n",
    "class MicrosoftDataReader(object):\n",
    "    \n",
    "    def __init__(self, file_path, num_session, split_ratio=0.8):\n",
    "        self.samples = {}\n",
    "        self.num_session = num_session\n",
    "        self.split_ratio = split_ratio\n",
    "        \n",
    "        session_read = 0\n",
    "        last_qid = None\n",
    "        print(\"---Start reading file for {} sessions---\".format(num_session))\n",
    "        with open(file_path) as f:\n",
    "            batched_samples = []\n",
    "            while session_read < num_session:\n",
    "                line = f.readline()\n",
    "                tokens = line.split(\" \")\n",
    "                label = float(tokens[0])\n",
    "                qid = int(tokens[1].split(\":\")[-1])\n",
    "                feature = [float(token.split(\":\")[-1]) for token in tokens[2:-1]]\n",
    "                if qid != last_qid:\n",
    "                    if last_qid is not None:\n",
    "                        session_read += 1\n",
    "                        self.samples[last_qid] = batched_samples\n",
    "                        batched_samples = []\n",
    "                        last_qid = qid\n",
    "                    else:\n",
    "                        last_qid = qid\n",
    "                batched_samples.append(Sample(label, feature))\n",
    "        print(\"---End reading file---\")\n",
    "        \n",
    "        all_sessions = [key for key in self.samples.keys()]\n",
    "        np.random.shuffle(all_sessions)\n",
    "        self.train = all_sessions[:int(len(all_sessions) * self.split_ratio)]\n",
    "        self.test = all_sessions[int(len(all_sessions) * self.split_ratio):]\n",
    "        \n",
    "#         print(\"---Training Sessions---\")\n",
    "#         print(self.train)\n",
    "#         print(\"---Testing Sessions---\")\n",
    "#         print(self.test)\n",
    "        \n",
    "    def read_training_samples(self):\n",
    "        for qid in self.train:\n",
    "            yield self.samples[qid]\n",
    "    \n",
    "    def read_testing_samples(self):\n",
    "        for qid in self.test:\n",
    "            yield self.samples[qid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Start reading file for 10 sessions---\n",
      "---End reading file---\n",
      "---Training Sessions---\n",
      "[46, 121, 31, 136, 106, 91, 1, 76]\n",
      "---Testing Sessions---\n",
      "[16, 61]\n",
      "120\n",
      "54\n",
      "92\n",
      "172\n",
      "23\n",
      "74\n",
      "86\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "reader = MicrosoftDataReader('/Users/yangpei/Downloads/MSLR-WEB10K/Fold1/train.txt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use nn module and optim module\n",
    "N, D_in, H, D_out = 4, 100, 100, 1\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.tensor([0, 1, 0, 1], device=device, dtype=torch.float).view(N, 1)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = LambdaLoss.apply\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(50):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
